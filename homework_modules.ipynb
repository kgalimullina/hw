{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3MeKai5Xj6eX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwlFrG-Tj6eY"
      },
      "source": [
        "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W8BLmtZ3j6eZ"
      },
      "outputs": [],
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box)\n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`:\n",
        "\n",
        "        output = module.forward(input)\n",
        "\n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule.\n",
        "\n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "\n",
        "        This includes\n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "\n",
        "        Make sure to both store the data in `output` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.output = input\n",
        "        # return self.output\n",
        "\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input.\n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "\n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "\n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.gradInput = gradOutput\n",
        "        # return self.gradInput\n",
        "\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Module\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKRkIjT8j6eZ"
      },
      "source": [
        "# Sequential container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb98PPpJj6ea"
      },
      "source": [
        "**Define** a forward and backward pass procedures.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7y2lav4dj6ea"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "         This class implements a container, which processes `input` data sequentially.\n",
        "\n",
        "         `input` is processed by each module (layer) in self.modules consecutively.\n",
        "         The resulting array is called `output`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "\n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Adds a module to the container.\n",
        "        \"\"\"\n",
        "        self.modules.append(module)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Basic workflow of FORWARD PASS:\n",
        "\n",
        "            y_0    = module[0].forward(input)\n",
        "            y_1    = module[1].forward(y_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(y_{n-2})\n",
        "\n",
        "\n",
        "        Just write a little loop.\n",
        "        \"\"\"\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = input\n",
        "        for module in self.modules:\n",
        "            self.output = module.forward(self.output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Workflow of BACKWARD PASS:\n",
        "\n",
        "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
        "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(y_0, g_2)\n",
        "            gradInput = module[0].backward(input, g_1)\n",
        "\n",
        "\n",
        "        !!!\n",
        "\n",
        "        To ech module you need to provide the input, module saw while forward pass,\n",
        "        it is used while computing gradients.\n",
        "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass)\n",
        "        and NOT `input` to this Sequential module.\n",
        "\n",
        "        !!!\n",
        "\n",
        "        \"\"\"\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = gradOutput\n",
        "        for i in range(len(self.modules)-1, 0, -1):\n",
        "            self.gradInput = self.modules[i].backward(self.modules[i-1].output, self.gradInput)\n",
        "        self.gradInput = self.modules[0].backward(input, self.gradInput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getParameters() for x in self.modules]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Should gather all gradients w.r.t parameters in a list.\n",
        "        \"\"\"\n",
        "        return [x.getGradParameters() for x in self.modules]\n",
        "\n",
        "    def __repr__(self):\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "\n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "        for module in self.modules:\n",
        "            module.train()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Propagates training parameter through all modules\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "        for module in self.modules:\n",
        "            module.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfXdYfO4j6ea"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuwvBkuNj6ea"
      },
      "source": [
        "## 1 (0.2). Linear transform layer\n",
        "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
        "- input:   **`batch_size x n_feats1`**\n",
        "- output: **`batch_size x n_feats2`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D0uoyqkpj6ea"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    A module which applies a linear transformation\n",
        "    A common name is fully-connected layer, InnerProductLayer in caffe.\n",
        "\n",
        "    The module should work with 2D input of shape (n_samples, n_feature).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # This is a nice initialization\n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "\n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.add(np.dot(input,np.transpose(self.W)), self.b)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.dot(gradOutput, self.W)\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradW = np.dot(np.transpose(gradOutput),input)\n",
        "        self.gradb = np.dot(np.ones(input.shape[0]),gradOutput)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNOnHXZJj6eb"
      },
      "source": [
        "## 2. (0.2) SoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
        "\n",
        "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": true,
        "id": "VIValI0hj6eb"
      },
      "outputs": [],
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "\n",
        "        np.exp(self.output, out=self.output)\n",
        "        np.divide(self.output, np.sum(self.output, axis=1, keepdims=True), out=self.output)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.zeros_like(gradOutput)\n",
        "        np.subtract(gradOutput, np.sum(np.multiply(gradOutput, self.output), axis=1, keepdims=True), out=self.gradInput)\n",
        "        np.multiply(self.gradInput, self.output, out=self.gradInput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftMax\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy3DJjynj6eb"
      },
      "source": [
        "## 3. (0.2) LogSoftMax\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
        "\n",
        "The main goal of this layer is to be used in computation of log-likelihood loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "Xo7DRdAJj6eb"
      },
      "outputs": [],
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "        super(LogSoftMax, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # start with normalization for numerical stability\n",
        "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        # the quation is same for the shifted input as logsoftmax(x_i) = log(sofrmax(x_i))\n",
        "        np.subtract(self.output, np.log(np.sum(np.exp(self.output), axis=1, keepdims=True)), out=self.output)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.zeros_like(gradOutput)\n",
        "        np.exp(self.output, out=self.gradInput)\n",
        "        np.multiply(self.gradInput, -np.sum(gradOutput, axis=1, keepdims=True), out=self.gradInput)\n",
        "        np.add(self.gradInput, gradOutput, out=self.gradInput)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"LogSoftMax\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP5QdmmPj6eb"
      },
      "source": [
        "## 4. (0.3) Batch normalization\n",
        "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
        "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\n",
        "```\n",
        "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
        "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
        "```\n",
        "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance.\n",
        "\n",
        "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "fGTTDqVgj6eb"
      },
      "outputs": [],
      "source": [
        "class BatchNormalization(Module):\n",
        "    EPS = 1e-3\n",
        "    def __init__(self, alpha = 0.):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = None\n",
        "        self.moving_variance = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        # use self.EPS please\n",
        "        self.n = input.shape[0]\n",
        "        self.output = np.zeros_like(input)\n",
        "        if self.training == True:\n",
        "            self.batch_mean = np.mean(input, axis=0)\n",
        "            self.batch_variance = np.var(input, axis=0)\n",
        "            self.moving_mean = self.moving_mean * self.alpha + self.batch_mean * (1 - self.alpha)\n",
        "            self.moving_variance = self.moving_variance * self.alpha + self.batch_variance * (1 - self.alpha)\n",
        "            np.divide(np.subtract(input,self.batch_mean),np.sqrt(np.add(self.batch_variance,self.EPS)), out = self.output)\n",
        "        else:\n",
        "            np.divide(np.subtract(input,self.moving_mean),np.sqrt(np.add(self.moving_variance,self.EPS)), out = self.output)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "        normalized = np.zeros_like(input)\n",
        "        np.divide(np.subtract(input,self.batch_mean),(np.sqrt(np.add(self.batch_variance,self.EPS))), out = normalized)\n",
        "        np.multiply(np.divide(1,np.multiply(np.sqrt(np.add(self.batch_variance,self.EPS)),self.n)),np.subtract(np.subtract(np.multiply(self.n,gradOutput),gradOutput.sum(axis=0)),np.multiply(normalized,np.sum(np.multiply(gradOutput,normalized), axis=0))), out = self.gradInput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "8XUS3Lt-j6eb"
      },
      "outputs": [],
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "\n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA5zjM3jj6eb"
      },
      "source": [
        "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gackeo1cj6eb"
      },
      "source": [
        "## 5. (0.3) Dropout\n",
        "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
        "\n",
        "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
        "\n",
        "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NmLQV3jXj6eb"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.zeros(input.shape)\n",
        "        self.mask = np.random.binomial(1, 1. - self.p, input.shape)\n",
        "        if self.training == True:\n",
        "            np.multiply(input, self.mask, out = self.output)\n",
        "            np.divide(self.output,1. - self.p, out = self.output)\n",
        "        else:\n",
        "            self.output = input\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.zeros(input.shape)\n",
        "        if self.training == True:\n",
        "            np.multiply(gradOutput, self.mask, out = self.gradInput)\n",
        "            np.divide(self.gradInput,(1. - self.p), out = self.gradInput)\n",
        "        else:\n",
        "            self.gradInput = gradOutput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Dropout\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. (2.0) Conv2d\n",
        "Implement [**Conv2d**](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). Use only this list of parameters: (in_channels, out_channels, kernel_size, stride, padding, bias, padding_mode) and fix dilation=1 and groups=1."
      ],
      "metadata": {
        "id": "-WHGIqJFlhz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.stride_tricks import sliding_window_view"
      ],
      "metadata": {
        "id": "QLR9U0NJnugD"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, bias=True, padding_mode='zeros'):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size\n",
        "        self.stride = (stride, stride) if isinstance(stride, int) else stride\n",
        "\n",
        "        if padding == 'same':\n",
        "            self.padding = ((self.kernel_size[0] - 1) // 2, (self.kernel_size[1] - 1) // 2)\n",
        "        else:\n",
        "            self.padding = (padding, padding) if isinstance(padding, int) else padding\n",
        "\n",
        "        self.has_bias = bias\n",
        "        self.padding_mode = padding_mode\n",
        "        self.training = True\n",
        "\n",
        "        # Initialize weights\n",
        "        std = np.sqrt(2.0 / (in_channels * self.kernel_size[0] * self.kernel_size[1]))\n",
        "        self.weight = np.random.normal(0, std,\n",
        "                                     size=(out_channels, in_channels,\n",
        "                                           self.kernel_size[0], self.kernel_size[1]))\n",
        "        self.gradW = np.zeros_like(self.weight)\n",
        "\n",
        "        if self.has_bias:\n",
        "            self.bias = np.zeros(out_channels)\n",
        "            self.gradb = np.zeros_like(self.bias)\n",
        "\n",
        "        # Set padding mode\n",
        "        if padding_mode == 'zeros':\n",
        "            self.pad_mode = 'constant'\n",
        "            self.pad_value = 0\n",
        "        elif padding_mode == 'replicate':\n",
        "            self.pad_mode = 'edge'\n",
        "        elif padding_mode == 'reflect':\n",
        "            self.pad_mode = 'reflect'\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "        return self\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "        return self\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        batch_size = input.shape[0]\n",
        "        in_h, in_w = input.shape[2], input.shape[3]\n",
        "        k_h, k_w = self.kernel_size\n",
        "        s_h, s_w = self.stride\n",
        "        p_h, p_w = self.padding\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        out_h = (in_h + 2 * p_h - k_h) // s_h + 1\n",
        "        out_w = (in_w + 2 * p_w - k_w) // s_w + 1\n",
        "\n",
        "        # Apply padding\n",
        "        if self.pad_mode == 'constant':\n",
        "            self.input_pad = np.pad(input,\n",
        "                                  ((0, 0), (0, 0),\n",
        "                                   (p_h, p_h), (p_w, p_w)),\n",
        "                                  mode=self.pad_mode,\n",
        "                                  constant_values=self.pad_value)\n",
        "        else:\n",
        "            self.input_pad = np.pad(input,\n",
        "                                  ((0, 0), (0, 0),\n",
        "                                   (p_h, p_h), (p_w, p_w)),\n",
        "                                  mode=self.pad_mode)\n",
        "\n",
        "        self.output = np.zeros((batch_size, self.out_channels, out_h, out_w))\n",
        "\n",
        "        # Perform convolution\n",
        "        for n in range(batch_size):\n",
        "            for c_out in range(self.out_channels):\n",
        "                for h in range(out_h):\n",
        "                    for w in range(out_w):\n",
        "                        h_start = h * s_h\n",
        "                        w_start = w * s_w\n",
        "                        receptive_field = self.input_pad[n, :,\n",
        "                                                       h_start:h_start+k_h,\n",
        "                                                       w_start:w_start+k_w]\n",
        "\n",
        "                        self.output[n, c_out, h, w] = np.sum(\n",
        "                            receptive_field * self.weight[c_out])\n",
        "\n",
        "                if self.has_bias:\n",
        "                    self.output[n, c_out] += self.bias[c_out]\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        batch_size = input.shape[0]\n",
        "        in_h, in_w = input.shape[2], input.shape[3]\n",
        "        k_h, k_w = self.kernel_size\n",
        "        s_h, s_w = self.stride\n",
        "        p_h, p_w = self.padding\n",
        "\n",
        "        self.gradInput = np.zeros_like(input)\n",
        "\n",
        "        # Compute gradient of input\n",
        "        for n in range(batch_size):\n",
        "            for c_out in range(self.out_channels):\n",
        "                for h in range(gradOutput.shape[2]):\n",
        "                    for w in range(gradOutput.shape[3]):\n",
        "                        h_start = h * s_h\n",
        "                        w_start = w * s_w\n",
        "                        grad_val = gradOutput[n, c_out, h, w]\n",
        "\n",
        "                        for c_in in range(self.in_channels):\n",
        "                            for kh in range(k_h):\n",
        "                                for kw in range(k_w):\n",
        "                                    h_in = h_start + kh - p_h\n",
        "                                    w_in = w_start + kw - p_w\n",
        "\n",
        "                                    if 0 <= h_in < in_h and 0 <= w_in < in_w:\n",
        "                                        self.gradInput[n, c_in, h_in, w_in] += (\n",
        "                                            grad_val * self.weight[c_out, c_in, kh, kw])\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        batch_size = input.shape[0]\n",
        "        k_h, k_w = self.kernel_size\n",
        "        s_h, s_w = self.stride\n",
        "\n",
        "        # Reset gradients\n",
        "        self.gradW.fill(0)\n",
        "        if self.has_bias:\n",
        "            self.gradb.fill(0)\n",
        "\n",
        "        # Compute gradients\n",
        "        for n in range(batch_size):\n",
        "            for c_out in range(self.out_channels):\n",
        "                for h in range(gradOutput.shape[2]):\n",
        "                    for w in range(gradOutput.shape[3]):\n",
        "                        h_start = h * s_h\n",
        "                        w_start = w * s_w\n",
        "                        receptive_field = self.input_pad[n, :,\n",
        "                                                       h_start:h_start+k_h,\n",
        "                                                       w_start:w_start+k_w]\n",
        "                        grad_val = gradOutput[n, c_out, h, w]\n",
        "\n",
        "                        self.gradW[c_out] += receptive_field * grad_val\n",
        "                        if self.has_bias:\n",
        "                            self.gradb[c_out] += grad_val\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        if self.has_bias:\n",
        "            self.gradb.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        if self.has_bias:\n",
        "            return [self.weight, self.bias]\n",
        "        return [self.weight]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        if self.has_bias:\n",
        "            return [self.gradW, self.gradb]\n",
        "        return [self.gradW]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"Conv2d(in_channels={self.in_channels}, \"\n",
        "                f\"out_channels={self.out_channels}, \"\n",
        "                f\"kernel_size={self.kernel_size}, \"\n",
        "                f\"stride={self.stride}, padding={self.padding}, \"\n",
        "                f\"bias={self.has_bias})\")"
      ],
      "metadata": {
        "id": "c1RjNoEXlOHP"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. (0.5) Implement [**MaxPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [**AvgPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html). Use only parameters like kernel_size, stride, padding (negative infinity for maxpool and zero for avgpool) and other parameters fixed as in framework."
      ],
      "metadata": {
        "id": "updUVZE9qixP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2d(Module):\n",
        "    def __init__(self, kernel_size, stride, padding):\n",
        "        super(MaxPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Применение max pooling с учетом padding\n",
        "        tensor_input = torch.tensor(input, dtype=torch.float32)\n",
        "\n",
        "        if self.padding > 0:\n",
        "            tensor_input = F.pad(tensor_input, (self.padding,) * 4, mode='constant', value=-float('inf'))\n",
        "\n",
        "        self.output = F.max_pool2d(tensor_input, kernel_size=self.kernel_size, stride=self.stride, padding=0).cpu().numpy()\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Вычисление градиента max pooling\n",
        "        tensor_input = torch.tensor(input, dtype=torch.float32)\n",
        "        tensor_grad = torch.tensor(gradOutput, dtype=torch.float32)\n",
        "\n",
        "        padded_shape = (\n",
        "            tensor_input.shape[0],\n",
        "            tensor_input.shape[1],\n",
        "            tensor_input.shape[2] + 2 * self.padding,\n",
        "            tensor_input.shape[3] + 2 * self.padding\n",
        "        )\n",
        "        grad_padded = torch.zeros(padded_shape, dtype=tensor_grad.dtype)\n",
        "\n",
        "        kh, kw = self.kernel_size, self.kernel_size\n",
        "        sh, sw = self.stride, self.stride\n",
        "\n",
        "        for b in range(tensor_grad.shape[0]):\n",
        "            for c in range(tensor_grad.shape[1]):\n",
        "                for i in range(tensor_grad.shape[2]):\n",
        "                    for j in range(tensor_grad.shape[3]):\n",
        "                        h_start, w_start = i * sh, j * sw\n",
        "                        region = tensor_input[b, c, h_start:h_start + kh, w_start:w_start + kw]\n",
        "                        max_idx = torch.argmax(region).item()\n",
        "                        max_pos = divmod(max_idx, region.shape[1])\n",
        "                        grad_padded[b, c, h_start + max_pos[0], w_start + max_pos[1]] += tensor_grad[b, c, i, j]\n",
        "\n",
        "        self.gradInput = (grad_padded[:, :, self.padding:-self.padding, self.padding:-self.padding] if self.padding > 0 else grad_padded).cpu().numpy()\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MaxPool2d\"\n",
        "\n",
        "class AvgPool2d(Module):\n",
        "    def __init__(self, kernel_size, stride, padding):\n",
        "        super(AvgPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Применение avg pooling с учетом padding\n",
        "        tensor_input = torch.tensor(input, dtype=torch.float32)\n",
        "        if self.padding > 0:\n",
        "            tensor_input = F.pad(tensor_input, (self.padding,) * 4, mode='constant', value=0)\n",
        "        self.output = F.avg_pool2d(tensor_input, kernel_size=self.kernel_size, stride=self.stride, padding=0).cpu().numpy()\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Вычисление градиента avg pooling\n",
        "        tensor_input = torch.tensor(input, dtype=torch.float32)\n",
        "        tensor_grad = torch.tensor(gradOutput, dtype=torch.float32)\n",
        "\n",
        "        padded_shape = (\n",
        "            tensor_input.shape[0],\n",
        "            tensor_input.shape[1],\n",
        "            tensor_input.shape[2] + 2 * self.padding,\n",
        "            tensor_input.shape[3] + 2 * self.padding\n",
        "        )\n",
        "        grad_padded = torch.zeros(padded_shape, dtype=tensor_grad.dtype)\n",
        "\n",
        "        kh, kw = self.kernel_size, self.kernel_size\n",
        "        sh, sw = self.stride, self.stride\n",
        "\n",
        "        for i in range(tensor_grad.shape[2]):\n",
        "            for j in range(tensor_grad.shape[3]):\n",
        "                h_start, w_start = i * sh, j * sw\n",
        "                grad_padded[:, :, h_start:h_start + kh, w_start:w_start + kw] += (tensor_grad[:, :, i:i+1, j:j+1] / (kh * kw))\n",
        "\n",
        "        self.gradInput = (grad_padded[:, :, self.padding:-self.padding, self.padding:-self.padding] if self.padding > 0 else grad_padded).cpu().numpy()\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AvgPool2d\"\n"
      ],
      "metadata": {
        "id": "tqpzHxl8BJSR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. (0.3) Implement **GlobalMaxPool2d** and **GlobalAvgPool2d**. They do not have testing and parameters are up to you but they must aggregate information within channels. Write test functions for these layers on your own."
      ],
      "metadata": {
        "id": "KTN5R3CwrukV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalMaxPool2d(Module):\n",
        "    def __init__(self):\n",
        "        super(GlobalMaxPool2d, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output, _ = torch.max(input.view(input.shape[0], input.shape[1], -1), dim=2)\n",
        "        self.output = self.output.unsqueeze(-1).unsqueeze(-1)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = torch.zeros_like(input)\n",
        "\n",
        "        # Найдем индексы максимальных значений\n",
        "        _, max_indices = torch.max(input.view(input.shape[0], input.shape[1], -1), dim=2)\n",
        "\n",
        "        self.gradInput.view(input.shape[0], input.shape[1], -1).scatter_(2, max_indices.unsqueeze(-1), gradOutput.view(input.shape[0], input.shape[1], 1))\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"GlobalMaxPool2d\"\n",
        "\n",
        "\n",
        "class GlobalAvgPool2d(Module):\n",
        "    def __init__(self):\n",
        "        super(GlobalAvgPool2d, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = torch.mean(input, dim=(2, 3), keepdim=True)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = torch.ones_like(input) * (gradOutput / (input.shape[2] * input.shape[3]))  # Нормализация\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"GlobalAvgPool2d\""
      ],
      "metadata": {
        "id": "62j0TyKIbNT6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. (0.2) Implement [**Flatten**](https://pytorch.org/docs/stable/generated/torch.flatten.html)"
      ],
      "metadata": {
        "id": "cYeBQDBhtViy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(Module):\n",
        "    def __init__(self, start_dim=0, end_dim=-1):\n",
        "        super(Flatten, self).__init__()\n",
        "        self.start_dim = start_dim\n",
        "        self.end_dim = end_dim\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        input_tensor = torch.as_tensor(input)\n",
        "\n",
        "        if self.end_dim == -1:\n",
        "            self.end_dim = input_tensor.dim() - 1\n",
        "\n",
        "        output_shape = input_tensor.shape[:self.start_dim] + (-1,) + input_tensor.shape[self.end_dim+1:]\n",
        "        self.output = input_tensor.view(output_shape).numpy()\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        input_tensor = torch.as_tensor(input)\n",
        "        gradOutput_tensor = torch.as_tensor(gradOutput)\n",
        "\n",
        "        input_shape = input_tensor.shape\n",
        "        gradInput = gradOutput_tensor.view(input_shape).numpy()\n",
        "\n",
        "        self.gradInput = gradInput\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Flatten\""
      ],
      "metadata": {
        "id": "SimPEMOFqhTQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o36vPHSSj6eb"
      },
      "source": [
        "# Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_pryRQIj6ec"
      },
      "source": [
        "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "sgm8bXjKj6ec"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ReLU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB0UHGagj6ec"
      },
      "source": [
        "## 10. (0.1) Leaky ReLU\n",
        "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "agwfkwO0j6ec"
      },
      "outputs": [],
      "source": [
        "class LeakyReLU(Module):\n",
        "    def __init__(self, slope=0.03):\n",
        "        super(LeakyReLU, self).__init__()\n",
        "        self.slope = slope\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = input.copy()\n",
        "        self.output[self.output < 0] = self.output[self.output < 0]*self.slope\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        pos0 = input >= 0\n",
        "        pos = gradOutput*(pos0)\n",
        "        boolneg = input < 0\n",
        "        neg = gradOutput*boolneg*self.slope\n",
        "        self.gradInput = pos + neg\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"LeakyReLU(slope={self.slope})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-STyecvj6ec"
      },
      "source": [
        "## 11. (0.1) ELU\n",
        "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "jJSzEu1mj6ec"
      },
      "outputs": [],
      "source": [
        "class ELU(Module):\n",
        "    def __init__(self, alpha = 1.0):\n",
        "        super(ELU, self).__init__()\n",
        "\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = input.copy()\n",
        "        self.output[self.output < 0] = (np.exp(self.output[self.output < 0])-1)*self.alpha\n",
        "        return  self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        pos0 = input >= 0\n",
        "        pos = gradOutput*(pos0)\n",
        "        boolneg = input < 0\n",
        "        neg = gradOutput*np.exp(input)*boolneg*self.alpha\n",
        "        self.gradInput = pos + neg\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ELU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn3C7KTqj6ec"
      },
      "source": [
        "## 12. (0.1) SoftPlus\n",
        "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "xcDPMssrj6ec"
      },
      "outputs": [],
      "source": [
        "class SoftPlus(Module):\n",
        "    def __init__(self):\n",
        "        super(SoftPlus, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        # Your code goes here. ################################################\n",
        "        self.output = np.log(np.exp(input)+1)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.divide(gradOutput, (np.exp(-input)+1))\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SoftPlus\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. (0.2) Gelu\n",
        "Implement [**Gelu**](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) activations."
      ],
      "metadata": {
        "id": "kw3PeZjOuo0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gelu(Module):\n",
        "    def __init__(self):\n",
        "        super(Gelu, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        tensor_input = torch.as_tensor(input, dtype=torch.float32)\n",
        "        self.output = F.gelu(tensor_input)\n",
        "        return self.output.cpu().numpy()\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        tensor_input = torch.as_tensor(input, dtype=torch.float32)\n",
        "        grad_output_tensor = torch.as_tensor(gradOutput, dtype=torch.float32)\n",
        "\n",
        "        sqrt_2 = torch.sqrt(torch.tensor(2.0))\n",
        "        cumulative = 0.5 * (1 + torch.erf(tensor_input / sqrt_2))\n",
        "        density = torch.exp(-0.5 * tensor_input ** 2) / torch.sqrt(torch.tensor(2.0 * torch.pi))\n",
        "        gelu_grad = cumulative + tensor_input * density\n",
        "\n",
        "        self.gradInput = (grad_output_tensor * gelu_grad).cpu().numpy()\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Gelu\"\n"
      ],
      "metadata": {
        "id": "SdieE0Dtuo8j"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YT9-nnqqkTvZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55p7UvPAj6ec"
      },
      "source": [
        "# Criterions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NFaxZaqj6ec"
      },
      "source": [
        "Criterions are used to score the models answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XGu45A8qj6ec"
      },
      "outputs": [],
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Criterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuU26xkpj6ec"
      },
      "source": [
        "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- target: **`batch_size x n_feats`**\n",
        "- output: **scalar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-i3VNuHhj6ec"
      },
      "outputs": [],
      "source": [
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LKLWNVj6ec"
      },
      "source": [
        "## 14. (0.2) Negative LogLikelihood criterion (numerically unstable)\n",
        "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n",
        "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
        "- input:   **`batch_size x n_feats`** - probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "die7KvW6j6ec"
      },
      "outputs": [],
      "source": [
        "class ClassNLLCriterionUnstable(Criterion):\n",
        "    EPS = 1e-15\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterionUnstable, self)\n",
        "        super(ClassNLLCriterionUnstable, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "\n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        self.N = input.shape[0]\n",
        "        self.output = np.negative(np.divide(np.sum(np.multiply(target, np.log(input_clamp))),self.N))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "\n",
        "        # Use this trick to avoid numerical errors\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.negative(np.divide(np.divide(target, input_clamp),self.N))\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterionUnstable\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHr_JbU5j6ec"
      },
      "source": [
        "## 15. (0.3) Negative LogLikelihood criterion (numerically stable)\n",
        "- input:   **`batch_size x n_feats`** - log probabilities\n",
        "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
        "- output: **scalar**\n",
        "\n",
        "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "v7N8bVP9j6ec"
      },
      "outputs": [],
      "source": [
        "class ClassNLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterion, self)\n",
        "        super(ClassNLLCriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        # Your code goes here. ################################################\n",
        "        self.N = input.shape[0]\n",
        "        self.output = np.negative(np.divide(np.sum(np.multiply(target, input)),self.N))\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        # Your code goes here. ################################################\n",
        "        self.gradInput = np.negative(np.divide(target,self.N))\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "E-ZnhKxaj6ed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-я часть задания: реализация слоев, лосей и функций активации - 5 баллов. \\\\\n",
        "2-я часть задания: реализация моделей на своих классах. Что должно быть:\n",
        "  1. Выберите оптимизатор и реализуйте его, чтоб он работал с вами классами. - 1 балл.\n",
        "  2. Модель для задачи мультирегрессии на выбраных вами данных. Использовать FCNN, dropout, batchnorm, MSE. Пробуйте различные фукнции активации. Для первой модели попробуйте большую, среднюю и маленькую модель. - 1 балл.\n",
        "  3. Модель для задачи мультиклассификации на MNIST. Использовать свёртки, макспулы, флэттэны, софтмаксы - 1 балла.\n",
        "  4. Автоэнкодер для выбранных вами данных. Должен быть на свёртках и полносвязных слоях, дропаутах, батчнормах и тд. - 2 балла. \\\\\n",
        "\n",
        "Дополнительно в оценке каждой модели будет учитываться:\n",
        "1. Наличие правильно выбранной метрики и лосс функции.\n",
        "2. Отрисовка графиков лосей и метрик на трейне-валидации. Проверка качества модели на тесте.\n",
        "3. Наличие шедулера для lr.\n",
        "4. Наличие вормапа.\n",
        "5. Наличие механизма ранней остановки и сохранение лучшей модели.\n",
        "6. Свитч лося (метрики) и оптимайзера."
      ],
      "metadata": {
        "id": "TC2Bf1PP2Ios"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "ixa62Fijs6Cm"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target.reshape(-1, 1)\n",
        "\n",
        "# Разделение и нормализация данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train = scaler_X.fit_transform(X_train)\n",
        "X_test = scaler_X.transform(X_test)\n",
        "y_train = scaler_y.fit_transform(y_train)\n",
        "y_test = scaler_y.transform(y_test)"
      ],
      "metadata": {
        "id": "DnBtTzk8tprB"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Вспомогательный класс для хранения параметров\n",
        "class Parameter:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.grad = np.zeros_like(data)\n",
        "\n",
        "# 2. класс MultiOutputRegressor\n",
        "class MultiOutputRegressor:\n",
        "    def __init__(self, input_size, output_size, model_type='medium'):\n",
        "        self.layers = []\n",
        "\n",
        "        if model_type == 'small':\n",
        "            self.layers.append(Linear(input_size, 64))\n",
        "            self.layers.append(ReLU())\n",
        "            self.layers.append(Linear(64, output_size))\n",
        "\n",
        "        elif model_type == 'medium':\n",
        "            self.layers.append(Linear(input_size, 128))\n",
        "            # Инициализируем BatchNorm с указанием размера\n",
        "            bn = BatchNormalization()\n",
        "            bn.moving_mean = np.zeros(128)  # 128 - размер выхода предыдущего слоя\n",
        "            bn.moving_variance = np.ones(128)\n",
        "            self.layers.append(bn)\n",
        "            self.layers.append(ReLU())\n",
        "            self.layers.append(Dropout(0.3))\n",
        "            self.layers.append(Linear(128, output_size))\n",
        "\n",
        "        else:  # large\n",
        "            self.layers.append(Linear(input_size, 256))\n",
        "            # Первый BatchNorm\n",
        "            bn1 = BatchNormalization()\n",
        "            bn1.moving_mean = np.zeros(256)\n",
        "            bn1.moving_variance = np.ones(256)\n",
        "            self.layers.append(bn1)\n",
        "            self.layers.append(ReLU())\n",
        "            self.layers.append(Dropout(0.4))\n",
        "\n",
        "            self.layers.append(Linear(256, 128))\n",
        "            # Второй BatchNorm\n",
        "            bn2 = BatchNormalization()\n",
        "            bn2.moving_mean = np.zeros(128)\n",
        "            bn2.moving_variance = np.ones(128)\n",
        "            self.layers.append(bn2)\n",
        "            self.layers.append(ReLU())\n",
        "            self.layers.append(Dropout(0.3))\n",
        "\n",
        "            self.layers.append(Linear(128, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        for layer in reversed(self.layers):\n",
        "            grad_output = layer.backward(layer.output if hasattr(layer, 'output') else grad_output, grad_output)\n",
        "        return grad_output\n",
        "\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'getParameters'):\n",
        "                layer_params = layer.getParameters()\n",
        "                if layer_params:  # Если есть параметры\n",
        "                    params.extend([Parameter(p) if not isinstance(p, Parameter) else p for p in layer_params])\n",
        "        return params\n",
        "\n",
        "    def train(self):\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'training'):\n",
        "                layer.training = True\n",
        "\n",
        "    def eval(self):\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'training'):\n",
        "                layer.training = False\n",
        "\n",
        "# 3. Adam оптимизатор\n",
        "class AdamOptimizer:\n",
        "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.t = 0\n",
        "\n",
        "        self.m = [np.zeros_like(p.data) for p in self.params]\n",
        "        self.v = [np.zeros_like(p.data) for p in self.params]\n",
        "\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        for i, param in enumerate(self.params):\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * param.grad\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (param.grad ** 2)\n",
        "\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            param.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.params:\n",
        "            param.grad = np.zeros_like(param.grad)\n",
        "\n",
        "# 4. Функция обучения\n",
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32, lr=0.01):\n",
        "    optimizer = AdamOptimizer(model.parameters(), lr=lr)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # Перемешивание данных\n",
        "        perm = np.random.permutation(len(X_train))\n",
        "\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            batch_idx = perm[i:i+batch_size]\n",
        "            X_batch = X_train[batch_idx]\n",
        "            y_batch = y_train[batch_idx].reshape(-1, 1)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model.forward(X_batch)\n",
        "            loss = np.mean((output - y_batch)**2)\n",
        "\n",
        "            # Backward pass\n",
        "            grad_output = 2 * (output - y_batch) / len(y_batch)\n",
        "            model.backward(grad_output)\n",
        "\n",
        "            # Обновление параметров\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss * len(y_batch)\n",
        "\n",
        "        # Валидация\n",
        "        model.eval()\n",
        "        val_output = model.forward(X_val)\n",
        "        val_loss = np.mean((val_output - y_val.reshape(-1, 1))**2)\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(X_train)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "        elif epoch > 10:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# 5. Загрузка и подготовка данных\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 6. Обучение моделей\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "\n",
        "print(\"Training small model...\")\n",
        "small_model = MultiOutputRegressor(input_size, output_size, 'small')\n",
        "small_losses = train_model(small_model, X_train, y_train, X_val, y_val, lr=0.01)\n",
        "\n",
        "print(\"\\nTraining medium model...\")\n",
        "medium_model = MultiOutputRegressor(input_size, output_size, 'medium')\n",
        "medium_losses = train_model(medium_model, X_train, y_train, X_val, y_val, lr=0.005)\n",
        "\n",
        "print(\"\\nTraining large model...\")\n",
        "large_model = MultiOutputRegressor(input_size, output_size, 'large')\n",
        "large_losses = train_model(large_model, X_train, y_train, X_val, y_val, lr=0.001)\n",
        "\n",
        "# 7. Оценка моделей\n",
        "def evaluate(model, X, y):\n",
        "    model.eval()\n",
        "    preds = model.forward(X)\n",
        "    return mean_squared_error(y, preds), r2_score(y, preds)\n",
        "\n",
        "print(\"\\nTest Results:\")\n",
        "for name, model in [('Small', small_model), ('Medium', medium_model), ('Large', large_model)]:\n",
        "    mse, r2 = evaluate(model, X_test, y_test)\n",
        "    print(f\"{name} Model - MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
        "\n",
        "# 8. Визуализация\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(small_losses[0], label='Small')\n",
        "plt.plot(medium_losses[0], label='Medium')\n",
        "plt.plot(large_losses[0], label='Large')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(small_losses[1], label='Small')\n",
        "plt.plot(medium_losses[1], label='Medium')\n",
        "plt.plot(large_losses[1], label='Large')\n",
        "plt.title('Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fJGV9Pq75kz_",
        "outputId": "d78de5a9-bb4f-42cf-9f7f-55056542b1ca"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training small model...\n",
            "Epoch 1/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 2/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 3/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 4/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 5/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 6/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 7/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 8/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 9/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 10/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 11/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Epoch 12/50, Train Loss: 5.5407, Val Loss: 5.6631\n",
            "Early stopping\n",
            "\n",
            "Training medium model...\n",
            "Epoch 1/50, Train Loss: 4.8375, Val Loss: 4.6632\n",
            "Epoch 2/50, Train Loss: 4.7960, Val Loss: 5.1102\n",
            "Epoch 3/50, Train Loss: 4.8022, Val Loss: 4.9240\n",
            "Epoch 4/50, Train Loss: 4.8281, Val Loss: 4.9016\n",
            "Epoch 5/50, Train Loss: 4.8265, Val Loss: 5.0400\n",
            "Epoch 6/50, Train Loss: 4.8164, Val Loss: 5.2697\n",
            "Epoch 7/50, Train Loss: 4.8313, Val Loss: 4.9148\n",
            "Epoch 8/50, Train Loss: 4.8172, Val Loss: 5.0830\n",
            "Epoch 9/50, Train Loss: 4.8201, Val Loss: 5.2108\n",
            "Epoch 10/50, Train Loss: 4.8124, Val Loss: 4.7494\n",
            "Epoch 11/50, Train Loss: 4.8216, Val Loss: 4.8483\n",
            "Epoch 12/50, Train Loss: 4.8236, Val Loss: 5.0252\n",
            "Early stopping\n",
            "\n",
            "Training large model...\n",
            "Epoch 1/50, Train Loss: 6.6102, Val Loss: 7.3669\n",
            "Epoch 2/50, Train Loss: 6.6382, Val Loss: 6.8272\n",
            "Epoch 3/50, Train Loss: 6.6348, Val Loss: 6.7915\n",
            "Epoch 4/50, Train Loss: 6.6316, Val Loss: 6.1417\n",
            "Epoch 5/50, Train Loss: 6.6268, Val Loss: 7.5214\n",
            "Epoch 6/50, Train Loss: 6.6074, Val Loss: 7.4943\n",
            "Epoch 7/50, Train Loss: 6.5856, Val Loss: 6.6190\n",
            "Epoch 8/50, Train Loss: 6.6325, Val Loss: 7.5248\n",
            "Epoch 9/50, Train Loss: 6.6209, Val Loss: 7.1857\n",
            "Epoch 10/50, Train Loss: 6.6227, Val Loss: 6.9780\n",
            "Epoch 11/50, Train Loss: 6.6165, Val Loss: 7.2657\n",
            "Epoch 12/50, Train Loss: 6.6308, Val Loss: 6.6615\n",
            "Early stopping\n",
            "\n",
            "Test Results:\n",
            "Small Model - MSE: 5.5105, R2: -3.2052\n",
            "Medium Model - MSE: 4.9034, R2: -2.7419\n",
            "Large Model - MSE: 6.3607, R2: -3.8540\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-usArGMo4K8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}